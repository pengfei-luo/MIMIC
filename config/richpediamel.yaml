run_name: RichpediaMEL
seed: 43
pretrained_model: 'openai/clip-vit-base-patch32'
lr: 1e-5


data:
  num_entity: 160933
  kb_img_folder: /YOUR_PATH/RichpediaMEL/kb_image
  mention_img_folder: /YOUR_PATH/RichpediaMEL/mention_image
  qid2id: /YOUR_PATH/RichpediaMEL/qid2id.json
  entity: /YOUR_PATH/RichpediaMEL/kb_entity.json
  train_file: /YOUR_PATH/RichpediaMEL/RichpediaMEL_train.json
  dev_file: /YOUR_PATH/RichpediaMEL/RichpediaMEL_dev.json
  test_file: /YOUR_PATH/RichpediaMEL/RichpediaMEL_test.json

  batch_size: 128
  num_workers: 8
  text_max_length: 40

  eval_chunk_size: 6000
  eval_batch_size: 20
  embed_update_batch_size: 512


model:
  input_hidden_dim: 512
  input_image_hidden_dim: 768
  hidden_dim: 96
  dv: 96
  dt: 512
  TGLU_hidden_dim: 96
  IDLU_hidden_dim: 96
  CMFU_hidden_dim: 96


trainer:
  accelerator: 'gpu'
  devices: 1
  max_epochs: 20
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 2
  log_every_n_steps: 30
